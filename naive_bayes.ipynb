{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "naive-bayes.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW03PKwt2tDR"
      },
      "source": [
        "#  Detection of Fake News"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CRKX96d2tDR"
      },
      "source": [
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import numpy as np\n",
        "import math\n",
        "import operator\n",
        "import re \n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TU_pKmU12tDS"
      },
      "source": [
        "data_dir = \"\"\n",
        "train_news_df = pd.read_csv(data_dir + 'fake_news_train.csv')\n",
        "\n",
        "train_index = [5547, 16639]\n",
        "test_index = [0 , 5546]\n",
        "\n",
        "# Train Data\n",
        "headlines_labels_arr = train_news_df[['title', 'text','label']][train_index[0]: train_index[1]].values.astype('U').tolist()\n",
        "headlines_arr_real =[]\n",
        "headlines_arr_fake =[]\n",
        "for line in headlines_labels_arr:\n",
        "    if(int(line[2]) == 0):\n",
        "        headlines_arr_real.append(str(line[0] + \" \" + line[1]))\n",
        "    if(int(line[2]) == 1):\n",
        "        headlines_arr_fake.append(str(line[0] + \" \" + line[1]))\n",
        "        \n",
        "\n",
        "#print(len(headlines_labels_arr)) #11092\n",
        "#print(len(headlines_arr_fake)) #5552\n",
        "#print(len(headlines_arr_real)) # 5540\n",
        "\n",
        "\n",
        "# Test Data\n",
        "test_headlines = []\n",
        "test_headlines.extend(headlines_labels_arr[test_index[0]: test_index[1]])\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOM0fxOJ2tDS",
        "outputId": "e27608f9-7e94-44f8-ba07-bb71e9ffed37"
      },
      "source": [
        "prob_fake = len(headlines_arr_fake) /len(headlines_labels_arr)\n",
        "prob_real = len(headlines_arr_real) /len(headlines_labels_arr)\n",
        "\n",
        "print(\"Probability of being fake = \", prob_fake)\n",
        "print(\"Probability of being real = \", prob_real)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Probability of being fake =  0.5005409304002885\n",
            "Probability of being real =  0.4994590695997115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD6wxzBV2tDS"
      },
      "source": [
        "create_vectorizer() function returns a list and dictionary. Size of the list equal to number of word in fake or real and it has information about how many times these word repeated. Dictionary has which word indicated which index of my array. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BibxfM02tDS"
      },
      "source": [
        "def create_vectorizer(arr, ngram):\n",
        "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(ngram, ngram))\n",
        "    X = vectorizer.fit_transform(arr).toarray()\n",
        "    X = np.array(np.sum(X, axis=0))\n",
        "    d1 = vectorizer.fit(arr).vocabulary_\n",
        "    return X, d1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rq01l9l2tDT"
      },
      "source": [
        "I extracted words that non-English and has non alphabetic character from my train dictionary to obtain more clear data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKCktW9m2tDT"
      },
      "source": [
        "def isEnglish(s):\n",
        "    try:\n",
        "        s.encode(encoding='utf-8').decode('ascii')\n",
        "    except UnicodeDecodeError:\n",
        "        return False\n",
        "    else:\n",
        "        return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXO7r7PO2tDT"
      },
      "source": [
        "# unigram fake data\n",
        "fake_arr_unigram, fake_d_unigram = create_vectorizer(headlines_arr_fake, 1)\n",
        "# unigram real data\n",
        "real_arr_unigram, real_d_unigram = create_vectorizer(headlines_arr_real, 1)\n",
        "\n",
        "\n",
        "# bigram fake data\n",
        "fake_arr_bigram, fake_d_bigram = create_vectorizer(headlines_arr_fake, 2)\n",
        "# bigram real data\n",
        "real_arr_bigram, real_d_bigram = create_vectorizer(headlines_arr_real, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJt3OBfO2tDT"
      },
      "source": [
        "count_of_unique_words() function finds words that only appear in fake or real news"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPlYtoIR2tDU"
      },
      "source": [
        "def count_of_unique_words(dict1,dict2):\n",
        "    common_value = dict1.keys() & dict2.keys()\n",
        "    return len(dict1.keys())+len(dict2.keys())-len(common_value)\n",
        "\n",
        "number_of_unique_word = count_of_unique_words(fake_dic_withcounts_unigram, real_dic_withcounts_unigram)\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction import stop_words\n",
        "stop_words = list(stop_words.ENGLISH_STOP_WORDS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDcJsc4S2tDU"
      },
      "source": [
        "Dictionaries named fake_dic_withcounts_unigram and real_dic_withcounts_unigram has words and count informations of them. There is not any words that have non-English character and non-alphabetic character. Also stop-words are extracted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZiUYvZ02tDU"
      },
      "source": [
        "fake_dic_withcounts_unigram = {}\n",
        "real_dic_withcounts_unigram = {}\n",
        "for k , v in fake_d_unigram.items():\n",
        "    if(isEnglish(k) == True and k.isalpha() and (k not in stop_words)):\n",
        "        fake_dic_withcounts_unigram[k]=fake_arr_unigram[v]\n",
        "    \n",
        "for k , v in real_d_unigram.items():\n",
        "    if(isEnglish(k) == True and k.isalpha() and (k not in stop_words)):\n",
        "        real_dic_withcounts_unigram[k]=real_arr_unigram[v]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iaUmGHD2tDV"
      },
      "source": [
        "## Part 1: Understanding the data\n",
        "\n",
        "The data-set consists of real and fake news headlines with 8315 real headline and about 8325 fake headlines. By looking at the number of word occurrences, we can calculate probability of being real or fake. Also there many of stop-words like (”of”, ”for”, ”on” ... etc). \n",
        "\n",
        "I listed three examples below that I believe that has huge affect for classifying the headlines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1CIhwXj2tDV"
      },
      "source": [
        "Function named find_uniques is for finding words that appear in fake news a lot when apperar in real news too little and vice versa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OM5W8xVm2tDV"
      },
      "source": [
        "def find_uniques(dic_fake, dic_real):\n",
        "    print(\"USED WORDS IN FAKE NEWS: \")\n",
        "    for k, v in dic_fake.items():\n",
        "        if(v > 500):\n",
        "            if (k not in dic_real.keys()):\n",
        "                print (\"Word: \", k )\n",
        "                print(\"Fake Count: \" , v)\n",
        "                print(\"Real Count: \" , 0)\n",
        "            elif ( dic_real.get(k) < 50):\n",
        "                print (\"Word: \", k )\n",
        "                print(\"Fake Count: \" , v)\n",
        "                print(\"Real Count: \" , dic_real.get(k,0))\n",
        "    print(\"\\nUSED WORDS IN REAL NEWS: \")\n",
        "    for k, v in dic_real.items():\n",
        "        if(v > 400):\n",
        "            if (k not in dic_fake.keys()):\n",
        "                print (\"Word: \", k )\n",
        "                print(\"Real Count: \" , v)\n",
        "                print(\"Fake Count: \" , 0)\n",
        "            elif ( dic_fake.get(k) < 20):\n",
        "                print (\"Word: \", k )\n",
        "                print(\"Real Count: \" , v)\n",
        "                print(\"Fake Count: \" , dic_fake.get(k))\n",
        "        \n",
        "                \n",
        "def display_dic(dic):\n",
        "    for k,v in dic.items():\n",
        "        print (k , \" : \" , v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p78JXjnC2tDV",
        "outputId": "942acfa3-7199-4224-ea4f-3c27e7f30225"
      },
      "source": [
        "find_uniques(fake_dic_withcounts_unigram, real_dic_withcounts_unigram)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "USED WORDS IN FAKE NEWS: \n",
            "Word:  www\n",
            "Fake Count:  614\n",
            "Real Count:  6\n",
            "Word:  http\n",
            "Fake Count:  706\n",
            "Real Count:  0\n",
            "Word:  der\n",
            "Fake Count:  620\n",
            "Real Count:  47\n",
            "Word:  que\n",
            "Fake Count:  1111\n",
            "Real Count:  15\n",
            "Word:  non\n",
            "Fake Count:  892\n",
            "Real Count:  14\n",
            "\n",
            "USED WORDS IN REAL NEWS: \n",
            "Word:  spicer\n",
            "Real Count:  485\n",
            "Fake Count:  3\n",
            "Word:  kushner\n",
            "Real Count:  414\n",
            "Fake Count:  8\n",
            "Word:  gorsuch\n",
            "Real Count:  442\n",
            "Fake Count:  0\n",
            "Word:  _____\n",
            "Real Count:  911\n",
            "Fake Count:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBF6Fr_K2tDW"
      },
      "source": [
        "#### List the 10 non-stopwords that most strongly predict that the news is fake"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wcm3UPfz2tDW",
        "outputId": "55edfb3d-31f3-4815-9cb2-57b9a277567f"
      },
      "source": [
        "print(\"---------------------------\\nMOST USED FAKE NONSTOP-WORDS: \\n---------------------------\\n\")\n",
        "res = dict(sorted(fake_dic_withcounts_unigram.items(), key=operator.itemgetter(1), reverse=True)[:10])\n",
        "display_dic(res)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------\n",
            "MOST USED FAKE NONSTOP-WORDS: \n",
            "---------------------------\n",
            "\n",
            "trump  :  11299\n",
            "clinton  :  10449\n",
            "people  :  8794\n",
            "hillary  :  7235\n",
            "said  :  6190\n",
            "just  :  5735\n",
            "new  :  5692\n",
            "like  :  5487\n",
            "time  :  5040\n",
            "world  :  5027\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KKViGxc2tDW"
      },
      "source": [
        "#### List the 10 non-stopwords that most strongly predict that the news is real"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sj_PxbEi2tDX",
        "outputId": "5512d17f-0ec2-465e-95f8-88720a0477dd"
      },
      "source": [
        "print(\"---------------------------\\nMOST USED REAL NONSTOP-WORDS: \\n---------------------------\\n\")\n",
        "res = dict(sorted(real_dic_withcounts_unigram.items(), key=operator.itemgetter(1), reverse=True)[:10])\n",
        "display_dic(res)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------\n",
            "MOST USED REAL NONSTOP-WORDS: \n",
            "---------------------------\n",
            "\n",
            "said  :  36768\n",
            "mr  :  34888\n",
            "trump  :  20825\n",
            "new  :  14145\n",
            "people  :  10817\n",
            "president  :  9149\n",
            "like  :  8593\n",
            "york  :  6913\n",
            "times  :  6841\n",
            "ms  :  6720\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dT3qr9I2tDX"
      },
      "source": [
        "As we have seen above, some words are seen many times in their cluster but we encounter to them in their opposite cluster very few.\n",
        "\n",
        "#### Fake Headlines' Words\n",
        "* que    : \n",
        "    * Fake Count: 1111  \n",
        "    * Real Count: 15\n",
        "    * Rate: 0.9975\n",
        "\n",
        "\n",
        "* non      : \n",
        "    * Fake Count: 892  \n",
        "    * Real Count: 14\n",
        "    * Rate: 0.9958\n",
        "\n",
        "\n",
        "* http   : \n",
        "    * Fake Count: 706  \n",
        "    * Real Count: 0 \n",
        "    * Rate: 0.9973\n",
        "\n",
        "#### Real Headlines' Words\n",
        "\n",
        "* kushner    : \n",
        "    * Real Count: 414  \n",
        "    * Fake Count: 8\n",
        "    * Rate: 0.9986\n",
        "\n",
        "\n",
        "* gorsuch      : \n",
        "    * Real Count: 442  \n",
        "    * Fake Count: 0\n",
        "    * Rate: 0.9998\n",
        "\n",
        "\n",
        "* spicer   : \n",
        "    * Real Count: 485 \n",
        "    * Fake Count: 3\n",
        "    * Rate: 0.9984\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjfJqDBv2tDX"
      },
      "source": [
        "## Part 2 : Implementing Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z0jmWdz2tDY"
      },
      "source": [
        "#Probability that a word being fake\n",
        "\n",
        "def fake_probability(word, arr_real, arr_fake, d_fake, d_real):\n",
        "    real_index = d_real.get(word, 0.0)\n",
        "    fake_index = d_fake.get(word, 0.0)\n",
        "    \n",
        "    if fake_index == 0 :\n",
        "        return (0 + 1) / ( len(d_fake) + number_of_unique_word)\n",
        "    else:\n",
        "        return (arr_fake[fake_index] + 1) / ( len(d_fake) )\n",
        "\n",
        "#Probability that a word being real\n",
        "\n",
        "def real_probability(word, arr_real, arr_fake, d_fake, d_real):\n",
        "    real_index = d_real.get(word, 0.0)\n",
        "    fake_index = d_fake.get(word, 0.0)\n",
        "    \n",
        "    if real_index == 0:\n",
        "        return (0 + 1) / ( len(d_real) + number_of_unique_word)\n",
        "    else:\n",
        "        return (arr_real[real_index] + 1) / (len(d_real) )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq98ii9P2tDY"
      },
      "source": [
        "# decide if naive bayes extacts stop-words by consideration\n",
        "extract_stopwords = False \n",
        "\n",
        "def naive_bayes(sentences_tuple, arr_real, arr_fake, d_real, d_fake,ngram):\n",
        "    print('Naive Bayes process started.\\n------------------------------------')\n",
        "    \n",
        "    true, false = 0, 0\n",
        "    real_prob, fake_prob = 0, 0\n",
        "    \n",
        "    for i,pair in enumerate(sentences_tuple):\n",
        "        real_prob = 0\n",
        "        fake_prob = 0\n",
        "        prediction = 0\n",
        "        \n",
        "        current_title = pair[0]\n",
        "        current_text = pair[1]\n",
        "        curent_body = current_title + \" \" + current_text\n",
        "        label = int(pair[2])\n",
        "        \n",
        "\n",
        "        for word in curent_title.split(' '):\n",
        "            word = word.lower()\n",
        "            word = re.sub(r'[^\\w\\s]','',word)\n",
        "            if (extract_stopwords):\n",
        "                if((word not in stop_words) and word.isnumeric() == False): \n",
        "                    fake_prob += math.log(fake_probability(word, arr_real, arr_fake, d_fake, d_real))\n",
        "                    real_prob += math.log(real_probability(word, arr_real, arr_fake, d_fake, d_real))\n",
        "            else:\n",
        "                fake_prob += math.log(fake_probability(word, arr_real, arr_fake, d_fake, d_real))\n",
        "                real_prob += math.log(real_probability(word, arr_real, arr_fake, d_fake, d_real))\n",
        "                \n",
        "\n",
        "        fake_prob += math.log(prob_fake)\n",
        "        real_prob += math.log(prob_real)\n",
        "\n",
        "        if real_prob >= fake_prob:\n",
        "            prediction = 0\n",
        "            #print(\"Prediction: Actual: \" + pair[1])\n",
        "            #print(\"Prediction: Found: \" + str(prediction))\n",
        "\n",
        "        else:\n",
        "            prediction = 1\n",
        "            #print(\"Prediction: Actual: \" + pair[1])\n",
        "            #print(\"Prediction: Found: \" +str(prediction))\n",
        "\n",
        "        if prediction == label:\n",
        "            true += 1\n",
        "        else:\n",
        "            false += 1\n",
        "            \n",
        "    accuracy = 100 * (true / (true + false))\n",
        "    \n",
        "    if ngram==1:\n",
        "        print(\"Unigram accuarcy -> \",accuracy)\n",
        "    else:\n",
        "        print(\"Bigram accuarcy -> \",accuracy)\n",
        "\n",
        "    print('Naive Bayes process finished\\n------------------------------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOFk3AlZ2tDY"
      },
      "source": [
        "def test( arr_real, arr_fake, d_fake, d_real, ngram):\n",
        "    if ngram == 1:\n",
        "        naive_bayes(test_headlines, arr_real, arr_fake, d_real, d_fake,  1)\n",
        "    elif ngram == 2:\n",
        "        naive_bayes(test_headlines, arr_real, arr_fake, d_real, d_fake,  2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTRN5qmN2tDY"
      },
      "source": [
        "### a) Analyzing effect of the words on prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo5DXrU42tDZ"
      },
      "source": [
        "###### PRESENCE\n",
        "\n",
        "P(class|word) = P(word|class) * P(class) / (P(word|fake)*P(fake) + P(word|real)*P(real))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5jiVq_b2tDZ"
      },
      "source": [
        "def presence(fake_dic,real_dic):\n",
        "    fake_presence={}\n",
        "    real_presence={}\n",
        "    for key,value in fake_dic.items():\n",
        "        #print(\"key: \", key)\n",
        "        payda=(fake_dic.get(key)/len(headlines_arr_fake))+(real_dic.get(key,0.00001)/len(headlines_arr_real))\n",
        "        probability=(fake_dic.get(key)/len(headlines_arr_fake))*prob_fake/payda\n",
        "        #print(\"probability: \", probability)\n",
        "        fake_presence[str(key)]=probability\n",
        "    for key,value in real_dic.items():\n",
        "        payda=(real_dic.get(key)/len(headlines_arr_real))+(fake_dic.get(key,0.00001)/len(headlines_arr_fake))\n",
        "        probability=((real_dic.get(key)/len(headlines_arr_real))*prob_real)/payda\n",
        "        real_presence[str(key)]=probability\n",
        "    return fake_presence,real_presence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Bk6rPD_2tDZ"
      },
      "source": [
        "fake_presences_unigram, real_presences_unigram = presence(fake_dic_withcounts_unigram, real_dic_withcounts_unigram)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DVGlrrz2tDZ"
      },
      "source": [
        "#### List the 10 words whose presence most strongly predicts that the news is fake."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZQhio232tDZ",
        "outputId": "920a1136-4114-4385-9973-dd4b15bf4558"
      },
      "source": [
        "dict(sorted(fake_presences_unigram.items(), key=operator.itemgetter(1), reverse=True)[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'http': 0.500540929387145,\n",
              " 'zu': 0.5005409266941779,\n",
              " 'html': 0.5005409265339137,\n",
              " 'kadzik': 0.5005409264701822,\n",
              " 'como': 0.5005409263362014,\n",
              " 'ist': 0.5005409257252602,\n",
              " 'trunews': 0.5005409255011151,\n",
              " 'auf': 0.5005409253631101,\n",
              " 'sich': 0.5005409251792715,\n",
              " 'nicht': 0.500540924940141}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpMGMDWs2tDa"
      },
      "source": [
        "#### List the 10 words whose presence most strongly predicts that the news is real."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5AOJvK02tDa",
        "outputId": "0ff993f6-ea27-44ab-a47d-1aa934ef97f6"
      },
      "source": [
        "dict(sorted(real_presences_unigram.items(), key=operator.itemgetter(1), reverse=True)[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'gorsuch': 0.49945899052429493,\n",
              " 'tillerson': 0.499458965886559,\n",
              " 'gorka': 0.4994588564818778,\n",
              " 'oesterlund': 0.49945883965679344,\n",
              " 'durst': 0.49945882855610607,\n",
              " 'haley': 0.49945882688219295,\n",
              " 'macron': 0.49945879865924103,\n",
              " 'awr': 0.49945878998915083,\n",
              " 'awrhawkins': 0.49945878773422825,\n",
              " 'pamkeynen': 0.49945878773422825}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEQXPyfe2tDa"
      },
      "source": [
        "\n",
        "###### ABSENCE\n",
        "\n",
        "P(~word|class) = 1- P(word|class)\n",
        "\n",
        "P(~word) = headlines without word / # headlines\n",
        "\n",
        "P(class|~word) = P(~word|class)*P(class)/P(~word)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyQC7hS92tDa"
      },
      "source": [
        "def absence_count(word):\n",
        "    c=0\n",
        "    for body in headlines_arr_fake:\n",
        "        if word not in body.split(\" \"):\n",
        "            c+=1\n",
        "    for body in headlines_arr_real:\n",
        "        if word not in body.split(\" \"):\n",
        "            c+=1\n",
        "    return c / (len(headlines_arr_real) + len(headlines_arr_fake)\n",
        "#whole_words_unigram = list(set(list(fake_d_unigram.keys())+list(real_d_unigram.keys())))\n",
        "\n",
        "def absence(fake_dic,real_dic):\n",
        "    fake_absence={}\n",
        "    real_absence={}\n",
        "    f = dict(sorted(fake_presences_unigram.items(), key=operator.itemgetter(1), reverse=False)[:20])\n",
        "    r = dict(sorted(real_presences_unigram.items(), key=operator.itemgetter(1), reverse=False)[:20])\n",
        "    for word in f:\n",
        "        payda = absence_count(word)\n",
        "        prob=( 1- fake_dic.get(word,0.1) / len(headlines_arr_fake) )*prob_fake/payda\n",
        "        fake_absence[word]=prob\n",
        "    for word  in r:\n",
        "        payda = absence_count(word)\n",
        "        prob = (1 - (real_dic.get(word, 0.1) / len(headlines_arr_real))) * prob_real / payda\n",
        "        real_absence[word] = prob\n",
        "\n",
        "    return fake_absence,real_absence\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ki1xFb92tDa"
      },
      "source": [
        "fake_absences_unigram, real_absences_unigram = absence(fake_dic_withcounts_unigram, real_dic_withcounts_unigram)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2UMi33L2tDa"
      },
      "source": [
        "#### List the 10 words whose absence most strongly predicts that the news is fake."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWlqXU032tDb",
        "outputId": "a597245f-7b8a-40ce-adb1-45b91d21a45a"
      },
      "source": [
        "dict(sorted(fake_absences_unigram.items(), key=operator.itemgetter(1), reverse=True)[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'weekdays': 0.5021587294006014,\n",
              " 'onstage': 0.5010036349433437,\n",
              " 'teammates': 0.5010036349433437,\n",
              " 'lineup': 0.5009245802243364,\n",
              " 'vouchers': 0.5007778304881579,\n",
              " 'playoffs': 0.5006762848754508,\n",
              " 'devos': 0.5004507753335737,\n",
              " 'redstone': 0.5004507753335737,\n",
              " 'mattis': 0.5004507753335737,\n",
              " 'streep': 0.5004507753335737}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrjLPtZM2tDb"
      },
      "source": [
        "#### List the 10 words whose absence most strongly predicts that the news is real."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Jlc-N4R2tDb",
        "outputId": "60824966-0479-4b6c-f64d-33a04f49a145"
      },
      "source": [
        "dict(sorted(real_absences_unigram.items(), key=operator.itemgetter(1), reverse=True)[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'para': 0.5056624680598154,\n",
              " 'th': 0.5054219488616982,\n",
              " 'por': 0.5044662055389851,\n",
              " 'su': 0.5039862215730946,\n",
              " 'una': 0.5039747686422555,\n",
              " 'und': 0.5024659883473656,\n",
              " 'als': 0.5020694035611349,\n",
              " 'neocon': 0.5020694035611349,\n",
              " 'oligarchy': 0.5014337002374616,\n",
              " 'te': 0.5013543504940082}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAP6fF-v2tDc"
      },
      "source": [
        "### b) Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07uMS3632tDc"
      },
      "source": [
        "fake_dic_withcounts_stopwords = {}\n",
        "real_dic_withcounts_stopwords = {}\n",
        "for k , v in fake_d_unigram.items():\n",
        "    if(isEnglish(k) == True and k.isalpha() and (k in stop_words)):\n",
        "        fake_dic_withcounts_stopwords[k] = fake_arr_unigram[v]\n",
        "    \n",
        "for k , v in real_d_unigram.items():\n",
        "    if(isEnglish(k) == True and k.isalpha() and (k in stop_words)):\n",
        "        real_dic_withcounts_stopwords[k] = real_arr_unigram[v]\n",
        "        \n",
        "def find_unique_stopwords(dic_fake, dic_real):\n",
        "    print(\"USED STOP WORDS IN FAKE NEWS: \\n--------------\\n\")\n",
        "    \n",
        "    for k, v in dic_fake.items():\n",
        "        if(( (v / (v + dic_real.get(k,0))) > 0.8) and v >100):\n",
        "            print (\"Word: \", k )\n",
        "            print(\"Fake Count: \" , v)\n",
        "            print(\"Real Count: \" , dic_real.get(k,0))\n",
        "            print(\"Rate: \" , (v / (v + dic_real.get(k,0))) )\n",
        "            print()\n",
        "            \n",
        "    print(\"\\nUSED STOP WORDS IN REAL NEWS: \\n--------------\\n\")\n",
        "    for k, v in dic_real.items():\n",
        "        if(( (v / (v + dic_fake.get(k,0))) > 0.7) and v >100):\n",
        "            print (\"Word: \", k )\n",
        "            print(\"Real Count: \" , v)\n",
        "            print(\"Fake Count: \" , dic_fake.get(k,0))\n",
        "            print(\"Rate: \", (v / (v + dic_fake.get(k,0))))\n",
        "            print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOKQgeEm2tDc",
        "outputId": "11698195-3bcc-4897-b4cb-5fd3d159e45a"
      },
      "source": [
        "dict(sorted(fake_dic_withcounts_stopwords.items(), key=operator.itemgetter(1), reverse=True)[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 200668,\n",
              " 'to': 96287,\n",
              " 'of': 95409,\n",
              " 'and': 88135,\n",
              " 'in': 64512,\n",
              " 'that': 47356,\n",
              " 'is': 43509,\n",
              " 'for': 31228,\n",
              " 'it': 28796,\n",
              " 'on': 25928}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HM9atDML2tDc",
        "outputId": "1baca1d0-e434-47a0-e4d1-ce0406fe63eb"
      },
      "source": [
        "dict(sorted(real_dic_withcounts_stopwords.items(), key=operator.itemgetter(1), reverse=True)[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 293054,\n",
              " 'to': 133638,\n",
              " 'of': 128600,\n",
              " 'and': 117304,\n",
              " 'in': 103705,\n",
              " 'that': 69114,\n",
              " 'for': 47298,\n",
              " 'on': 45354,\n",
              " 'he': 40916,\n",
              " 'is': 40311}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ewDB0mh2tDc"
      },
      "source": [
        "As we seen above the most used stop words are generally common for both real and fake news. This means that whether or not taking into consideration these words is just waste of time and space. For not gaze upon just these most used stop words, I observed some stopwords that proportion of being fake or real is high."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDjT-Ye42tDd",
        "outputId": "2b7425bd-a79b-48f1-ec8e-9266803fed81"
      },
      "source": [
        "find_unique_stopwords(fake_dic_withcounts_stopwords, real_dic_withcounts_stopwords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "USED STOP WORDS IN FAKE NEWS: \n",
            "--------------\n",
            "\n",
            "Word:  etc\n",
            "Fake Count:  368\n",
            "Real Count:  51\n",
            "Rate:  0.8782816229116945\n",
            "\n",
            "Word:  co\n",
            "Fake Count:  695\n",
            "Real Count:  28\n",
            "Rate:  0.9612724757952974\n",
            "\n",
            "Word:  de\n",
            "Fake Count:  3578\n",
            "Real Count:  851\n",
            "Rate:  0.8078573041318582\n",
            "\n",
            "Word:  un\n",
            "Fake Count:  1101\n",
            "Real Count:  117\n",
            "Rate:  0.9039408866995073\n",
            "\n",
            "Word:  con\n",
            "Fake Count:  368\n",
            "Real Count:  28\n",
            "Rate:  0.9292929292929293\n",
            "\n",
            "\n",
            "USED STOP WORDS IN REAL NEWS: \n",
            "--------------\n",
            "\n",
            "Word:  his\n",
            "Real Count:  27651\n",
            "Fake Count:  10162\n",
            "Rate:  0.7312564461957528\n",
            "\n",
            "Word:  had\n",
            "Real Count:  18218\n",
            "Fake Count:  6154\n",
            "Rate:  0.747497127851633\n",
            "\n",
            "Word:  he\n",
            "Real Count:  40916\n",
            "Fake Count:  13867\n",
            "Rate:  0.7468740302648632\n",
            "\n",
            "Word:  him\n",
            "Real Count:  7625\n",
            "Fake Count:  3106\n",
            "Rate:  0.7105581958810921\n",
            "\n",
            "Word:  last\n",
            "Real Count:  5991\n",
            "Fake Count:  2325\n",
            "Rate:  0.7204184704184704\n",
            "\n",
            "Word:  whether\n",
            "Real Count:  2346\n",
            "Fake Count:  999\n",
            "Rate:  0.7013452914798206\n",
            "\n",
            "Word:  seemed\n",
            "Real Count:  1019\n",
            "Fake Count:  194\n",
            "Rate:  0.8400659521846661\n",
            "\n",
            "Word:  she\n",
            "Real Count:  14746\n",
            "Fake Count:  6067\n",
            "Rate:  0.7084994955076155\n",
            "\n",
            "Word:  whose\n",
            "Real Count:  1427\n",
            "Fake Count:  580\n",
            "Rate:  0.7110114598903836\n",
            "\n",
            "Word:  sometimes\n",
            "Real Count:  890\n",
            "Fake Count:  369\n",
            "Rate:  0.7069102462271644\n",
            "\n",
            "Word:  nine\n",
            "Real Count:  444\n",
            "Fake Count:  170\n",
            "Rate:  0.7231270358306189\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e0wgsky2tDd"
      },
      "source": [
        "#### Stopwords that most strongly predict that the news is fake\n",
        "\n",
        "* con    : \n",
        "    * Fake Count: 368  \n",
        "    * Real Count: 15\n",
        "    * Rate: 0.9292\n",
        "\n",
        "\n",
        "* un      : \n",
        "    * Fake Count: 1101  \n",
        "    * Real Count: 117\n",
        "    * Rate: 0.9039\n",
        "\n",
        "\n",
        "* co   : \n",
        "    * Fake Count: 695  \n",
        "    * Real Count: 28 \n",
        "    * Rate: 0.9612"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vraxyt4e2tDd"
      },
      "source": [
        "#### Stopwords that most strongly predict that the news is real\n",
        "\n",
        "* seemed    : \n",
        "    * Real Count: 1019  \n",
        "    * Fake Count: 194\n",
        "    * Rate: 0.8400\n",
        "\n",
        "* whose      : \n",
        "    * Real Count: 1447  \n",
        "    * Fake Count: 580\n",
        "    * Rate: 0.7110\n",
        "\n",
        "* nine   : \n",
        "    * Real Count: 444  \n",
        "    * Fake Count: 170\n",
        "    * Rate: 0.7231"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPpJ8rIA2tDd"
      },
      "source": [
        "According to these results above, stopwords that most strongly predict that the news is real or fake are not too much by comparison with mostly used stopwords. By considering this inference I decided not to use stop-words in my model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIS6VMB92tDd"
      },
      "source": [
        "### c) Analyzing effect of the stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PYethls2tDd",
        "outputId": "2e9266d0-57d6-4383-d1a6-f3996403ef44"
      },
      "source": [
        "#Test unigram data with stopwords \n",
        "test(real_arr_unigram,fake_arr_unigram, fake_d_unigram, real_d_unigram,  1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive Bayes process started.\n",
            "------------------------------------\n",
            "Unigram accuarcy ->  83.88027407140282\n",
            "Naive Bayes process finished\n",
            "------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8qsSIBz2tDe"
      },
      "source": [
        "When I deactive affects of some stopwords and numeric characters over test accuracy, I clearly see it enhances my model. This process is increased my accuracy proportion of 10 percent. This is because of they don’t help us to find the context or the true meaning of a sentence. We are reducing the data set size is without any doubt and by doing this we can increase performance of our model. \n",
        "\n",
        "An example could be the following sentence: “Yes, There Are Paid Government Trolls On Social Media, Blogs, Forums And Websites”. When I remove the stop words, new sentence would be \"yes paid government trolls social media blogs forums websites\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMczQoHL2tDe",
        "outputId": "c3ddc94d-1004-4ac9-94af-b32df33cb9a4"
      },
      "source": [
        "#Test unigram data after extract stopwords \n",
        "extract_stopwords =True\n",
        "test(real_arr_unigram,fake_arr_unigram, fake_d_unigram, real_d_unigram,  1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive Bayes process started.\n",
            "------------------------------------\n",
            "Unigram accuarcy ->  93.32852506310854\n",
            "Naive Bayes process finished\n",
            "------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K69Tw3eQ2tDe",
        "outputId": "d822429f-6e2d-49e6-d0aa-7fa3fb0944f5"
      },
      "source": [
        "#Test bigram data after extract stopwords  \n",
        "test(real_arr_bigram, fake_arr_bigram, fake_d_bigram, real_d_bigram, 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive Bayes process started.\n",
            "------------------------------------\n",
            "Bigram accuarcy ->  49.98196898665705\n",
            "Naive Bayes process finished\n",
            "------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa96RFBK2tDe"
      },
      "source": [
        "In my implementation unigram accuracy is very high by comparison with bigram accuracy. I think using bigram data is not suitable for our problem. Because of that I decided to use unigram data in by problem."
      ]
    }
  ]
}